{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info 6010 Ex 1\n",
    "### 3 Feb 2019, covers lectures 2,3, due Thu night 13 Feb 2019\n",
    "\n",
    "[You should start with a new empty notebook, call it s1, and copy/paste only code that you need from cells in this notebook, the full problem statements are not necessary.<br>\n",
    "The first cell at the top should include your name, netid, and 'Ex1': use the menu button 2nd from right to change from code to markdown cell.]<br>\n",
    "Upload to usual course [upload site](https://pgcourse.infosci.cornell.edu/cgi-bin/probset.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This covers some simple text manipulation, to give a better handle on where everyone stands.\n",
    "\n",
    "The first part illustrates basic file manipulations, and other things from [lec2](https://piazza.com/class/k5vlbhuxrhq4oa?cid=9), including the Shakespeare power law (cell [29] of [lec2a.ipynb](http://nbviewer.jupyter.org/url/www.cs.cornell.edu/~ginsparg/6010/spr20/lec2a.ipynb)).\n",
    "\n",
    "The latter part applies the simple \"Naive Bayes\" methodology (to be continued in lec3) to construct a simple text classifier. (This is really a warmup, certainly  too elementary for some, but at the edge of feasibility for others so the text provides most of the framework, conveying the basic \"test set / training set\" machine learning methodology.\n",
    "There's an extra problem for students with prior [scikit-learn.org](https://scikit-learn.org/stable/) experience.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "\n",
    "An example used in [lec2b.ipynb](https://nbviewer.jupyter.org/url/www.cs.cornell.edu/~ginsparg/6010/spr20/lec2b.ipynb) involved building a Bernoulli Naive Bayes classifier based on 1000 abstracts each from a single physics category and a single biology category (1800 training texts, and 200 test texts). This dataset [Ex1data.py.gz](https://www.cs.cornell.edu/~ginsparg/6010/spr20/Ex1data.py.gz) expands on that with similar data from twelve categories. gunzip that file and use\n",
    "\n",
    "    from Ex1data import absdata\n",
    "\n",
    "to import absdata, which will then be a dictionary of twelve subject areas in fields of astrophysics (GA = galaxies), condensed matter, computer science (CV= computer vision, HC = human computer interactions, LG = machine learning), two closely related areas of high energy physics (phenomenology and theory), math, applied physics, computational physics, biology (NC= neurons and cognition), and quantum physics. The value of each is a list of the texts of 1000 recent abstracts, for a total of 12000 abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['astro-ph.GA', 'cond-mat.mes-hall', 'cs.CV', 'cs.HC', 'cs.LG', 'hep-ph', 'hep-th', 'math.AP', 'physics.app-ph', 'physics.comp-ph', 'q-bio.NC', 'quant-ph']\n"
     ]
    }
   ],
   "source": [
    "from Ex1data import absdata\n",
    "print (sorted(absdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n"
     ]
    }
   ],
   "source": [
    "print ([len(absdata[cat]) for cat in absdata])  #cat = \"category\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data-driven decision making related to individuals has become increasingly pervasive, but the issue concerning the potential discrimination has been raised by recent studies. In response, researchers have made efforts to propose and implement fairness measures and algorithms, but those efforts have not been translated to the real-world practice of data-driven decision making. As such, there is still an urgent need to create a viable tool to facilitate fair decision making. We propose FairSight, a visual analytic system to address this need; it is designed to achieve different notions of fairness in ranking decisions through identifying the required actions -- understanding, measuring, diagnosing and mitigating biases -- that together lead to fairer decision making. Through a case study and user study, we demonstrate that the proposed visual analytic and diagnostic modules in the system are effective in understanding the fairness-aware decision pipeline and obtaining more fair outcomes.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first cs.HC abstract\n",
    "absdata['cs.HC'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed for the Shakespeare case in lec2, it's necessary to tokenize, or split the texts into something like words. There for simplicity the text was split on whitespace using the `.split()` method, but that left punctuation characters.\n",
    "For a more useful spliting, words can be considered strings of lower case letters a-z plus apostrophes, and extracted as a list. For example, for the first cs.HC text above, and using a regular expression that finds all strings with a-z and ':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "re.findall(\"[a-z0-9']+\", absdata['cs.HC'][0].lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(`re.findall(\"[a-z0-9']+\",txt)` just finds all contiguous strings made up of the characters a-z or digits 0-9 or apostrophe ', and returns them as a list.)\n",
    "\n",
    "This will return a list of all the 'words' in that document (try it). For the time being, we only want to count the *number of documents* in which a word occurs (not the number of *occurrences* within the document), so we apply `set()` to the above list so that each word only appears once.\n",
    "Finally, it's convenient to use the Counter object (http://docs.python.org/2/library/collections.html), which is just a dictionary to accumulate counts for the words. The result is (try it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter( re.findall(\"[a-z0-9']+\", absdata['cs.HC'][0].lower()) ).most_common(20)\n",
    "# 20 most common, 'the' should appear 7 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#setup for inline plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt, numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Zipf's Law\n",
    "\n",
    "Start with a look at word distributions and power laws. The following cell iterates through the 2000 abstracts from cs.HC and cs.LG, and accumulates their word counts in the Counter object `thewords`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "thewords=Counter()\n",
    "for subj in ('cs.HC', 'cs.LG'):\n",
    "    for txt in absdata[subj]:\n",
    "       thewords += Counter( re.findall(\"[a-z0-9']+\", txt.lower()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13861"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#size of vocab\n",
    "len(thewords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at the top few words and their counts, using say (try it...)\n",
    "\n",
    "    thewords.most_common(40)\n",
    "\n",
    "(the most common word should be 'the', and it should occur more than 15000 times).\n",
    "`thewords` is a python dictionary (with some extra methods), so the counts themselves are contained in `thewords.values()`, use `sum(thewords.values())` on those to determine the total number of words in the \"corpus\" (It should be over 300,000.)\n",
    "\n",
    "The object of the part of the assignment is to plot the number of occurrences for the top 10000 words in your `thewords` dictionary on the y-axis, ordered so that the word with the largest number of occurrences is plotted first, the word with second largest number of occurrences is plotted second, and so on. This is known as a \"rank-frequency\" graph, since it plots the frequency of occurrence vs. the rank of the word.\n",
    "\n",
    "The first graph you plot might look something like cell [28] of [lec2a.ipynb](http://nbviewer.jupyter.org/url/www.cs.cornell.edu/~ginsparg/6010/spr20/lec2a.ipynb), with some words used very frequently, and most used less frequently. It's difficult to see the overall structure of the data in that cell since most of the values are so small. A standard method is to plot using logarithmic scales as in cell [29], so that a fixed distance along the axis corresponds to say a factor of 10 (rather than just adding 10). This can be implemented by adding `plt.xscale('log')` or `plt.yscale('log')`, or both, before or after the `plt.plot()` command; or by using `plt.loglog()` instead of `plt.plot()` (or `plt.semilogx()` or `plt.semilogy()` for only one or the other axis). They give the same type of plot.\n",
    "\n",
    "Your second plot should look something like this (except use real data, rather than the synthetic data used here, so it won't \"magically\" follow a straight line in log-log coordinates -- for this \"data\" automatic since it was generated artificially using the function 1/r for rank r):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "m=10000.\n",
    "#fake top10 (instead use the real top10):\n",
    "top10 = ['the', 'of', 'and', 'to', 'in', 'i', 'that', 'was', 'his', 'he']\n",
    "\n",
    "plt.figure(figsize=(7.5,7))\n",
    "r = 1+np.arange(10000)\n",
    "plt.loglog(r,m/r,'o') #fake data, use instead real data\n",
    "plt.xlim(.8,1e4)\n",
    "plt.ylim(1,)\n",
    "plt.plot(r,m/r,'-') #fake \"fit\" to data\n",
    "for x,y,w in zip(r,m/r,top10):\n",
    "    plt.text(x,.1*y**1.2,w,ha='center', fontsize=8)\n",
    "plt.legend(['\"data\"','\"fit\"']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above also uses the `plt.text()` command to label the top ten occurring words (where that top ten is not from the cs.HC/cs.LG data, but instead data for the [Gutenberg corpus](http://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/PG/2006/04/1-10000) ).\n",
    "Also plotted in orange is a \"fit\" to the data, where the fit in this case is exact since it's just the same 1/r function used to generate the data.\n",
    "\n",
    "**A.** The above is idealized data, the object of this problem is to see what the plot looks like for real data.\n",
    "\n",
    "For this part of the assignment, you should figure out how to extract the counts for the top 10000 words in your `thewords` dictionary, and plot those counts against the word ranks. You should annotate the top ten words on the graph with what they are (as above, though using the actual word rank in your dictionary).\n",
    "Finally you should try to fit the data you've plotted with a straight line, as above. (You can do this visually, by experimenting with values of `m`, or if more ambitious you can use a linear regression `scipy.stats.linregress()` to find the best straight line: by running `linregress()` on the `np.log()` of the x-values and y-values to determine the slope `a` and intercept `b` in log-log space, then plotting `np.exp(a*x+b)` which will render as a straight line. If you do the latter, just fit to the words ranked 7th through 3000th to avoid noisy data at the two ends.)\n",
    "\n",
    "Not to give too much of a spoiler, you should find that many of the data points do fall roughly along a straight line in log-log space (Zipf's law), even for this small amount of data (i.e., only the 2000 short abstracts).<br>\n",
    "(Feel free to try it for all 12000 abstracts, just change `for subj in ('cs.HC', 'cs.LG'):` in the above code to `for subj in absdata:` to iterate over all twelve.)\n",
    "\n",
    "**B.** Regarding the Oxford English Corpus (=OEC, more than 2 billion words), it says\n",
    "[here](https://en.wikipedia.org/wiki/Most_common_words_in_English): \"the first 25 words in the OEC make up about one-third of all printed material in English, and the first 100 words make up about half of all written English\".<br>\n",
    "For the corpus in this problem, `sum(thewords.values())` gives the total number of words (i.e., words counted with their multiplicities), and `len(thewords)` gives the number of *distinct* words (i.e., the size of the vocabulary).<br>\n",
    "*i)* What percentage of the corpus is made up of the top 10, top 25, and of the top 100 ranked words?<br>\n",
    "*ii)* For this corpus, what number of top ranked words make up 25%, 50%, 75% of the corpus?<br>\n",
    "*iii)* For each of the numbers in ii), what percentage of the vocabulary do they represent?<br>\n",
    "(e.g., if the top 100 words in a vocabulary of 10000 distinct words were used 50000 times in a corpus of size 100000 words, then that would mean the top ranked 1% of words constitutes 50% of the corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Naive Bayes text classifier\n",
    "\n",
    "Here you will train a simple binary classifier, for cs.HC = Human computer interactions vs. cs.LG = machine learning, based on the first 900 abstracts in each of those two categories, and test it on the last 100 \"test\" abstracts from each category. This follows the example given in [lec2b.ipynb](https://nbviewer.jupyter.org/url/www.cs.cornell.edu/~ginsparg/6010/spr20/lec2b.ipynb).\n",
    "\n",
    "Parts of the remainder of this notebook are useful only if it isn't clear how to proceed (and you're free to deviate in any way you want).\n",
    "\n",
    "For variety, now we'll count only the *number of documents* in which a word occurs (not the number of *occurrences* within the document). To this end, we apply set() to the list of words so that each word only appears once.\n",
    "Again using the `Counter()` object (http://docs.python.org/2/library/collections.html), the result of inserting `set()` is (try it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter(set(re.findall(\"[a-z0-9']+\",absdata['cs.HC'][0].lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number counts (number of documents in which word occurs) can be accumulated as was done for bvocab and pvocab in cell [10] of [lec2b.ipynb](https://nbviewer.jupyter.org/url/www.cs.cornell.edu/~ginsparg/6010/spr20/lec2b.ipynb), perhaps call them `HCvocab` and `LGvocab`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again using the `Counter()` object's convenient `most_common` method, you can look at the numbers for the most frequent words (where 900 would mean they occurred in all 900 training documents):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter(HCvocab).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or to see the 285th through 296th, use (try it, should give words appearing in only forty eight of the training documents):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Counter(HCvocab).most_common()[285:296]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "and similarly for `LGvocab` for the machine learning documents.<br>\n",
    "Finally you need to write the classifier code (will be roughly a grand total of less than 10 lines of code) to implement the Naive Bayes classifier (or repurpose the code from lec2b.ipynb). The formula to implement (for a flat prior $p({\\rm HC})=p({\\rm LG})=.5$) is\n",
    "$$\n",
    "p({\\rm HC}\\ |\\ words) = \\frac{p(words\\ |\\ {\\rm HC})\\,p({\\rm HC})}\n",
    "{p(words\\ |\\ {\\rm HC})\\,p({\\rm HC})+p(words\\ |\\ {\\rm LG})\\,p({\\rm LG})}\n",
    "\\approx\\frac{\\prod_i p(w_i\\ |\\ {\\rm HC})}{\\prod_i p(w_i\\ |\\ {\\rm HC})+\\prod_i p(w_i\\ |\\ {\\rm LG})}\n",
    "$$\n",
    "<br>\n",
    "This formula might look unfamiliar so let's try to unwrap it. (This was started in lec2, but ran out of time, so will continue in lec3.) The conditional word probabilities, e.g., $p(w_i\\ |\\ {\\rm HC})$ (\"the probability that word $w_i$ occurs if the abstract is labelled HC), are estimated based on the number of HC documents in which they occur in the training set. So if the word 'health' appears in 48 such documents, then we would estimate that $p($'health' $|\\ {\\rm HC}) = 48/1000$. The product signs $\\Pi_i$ in the above just mean that we multiply those probabilities for each word that appears in the test document.<br>\n",
    "Intuitively, the above formula considers the words in a text document, and if more of those words tend to occur preferentially in the HC abstracts in the training set, then the test abstract is determined to be more likely an HC abstract (because the numerator is more than half the denominator), and vice versa if its words tend to occur more often in the LG abstracts in the training set.\n",
    "<br><br>\n",
    "\n",
    "As in lec2b.ipynb, you can try using the full vocabulary, and then experiment with using the most common words, or the most discriminating.\n",
    "\n",
    "**A.** Train on the first 900 documents in each of the cs.HC and cs.LG abstracts (a total of 1800 documents), and test on the last 100 in each of the classes (a total of 200 documents).  Experiment with different size vocabularies, e.g., the full vocabulary, the 100 most common words, and the 100 most discriminating words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "**B.** As mentioned above, there are 10000 more abstracts in the Ex1data.py (for a total of 12 classes).\n",
    "\n",
    "Implement a four way classifier (cs.HC, cs.LG, cs.CV = computer vision, q-bio.NC = neurons and cognition), where for example the probability of cs.HC would now be:\n",
    "$$p({\\rm HC}\\ |\\ words) = \\frac{p(words\\ |\\ {\\rm HC})\\,p({\\rm HC})}{p(words\\ |\\ {\\rm HC})\\,p({\\rm HC}) + p(words\\ |\\ {\\rm LG})\\,p({\\rm LG}) + p(words\\ |\\ {\\rm CV})\\,p({\\rm CV}) + p(words\\ |\\ {\\rm NC})\\,p({\\rm NC})} \\approx \\frac{\\prod_i p(w_i\\ |\\ {\\rm HC})}{\\prod_i p(w_i\\ |\\ {\\rm HC}) + \\prod_i p(w_i\\ |\\ {\\rm LG}) + \\prod_i p(w_i\\ |\\ {\\rm CV}) + \\prod_i p(w_i\\ |\\ {\\rm NC})}$$\n",
    "<br>\n",
    "and similarly for cs.LG, cs.CV, and q-bio.NC. The predicted classification is the one with the highest probability.\n",
    "\n",
    "Train it on the first 900 abstracts in each of those four categories (a total of 3600 documents), and test on the last 100 from each of those four categories (a total of 400 documents).\n",
    "\n",
    "---\n",
    "**C. (optional)** Implement a twelve way classifier (all twelve classes in cell [1] above), \n",
    "trained on the first 900 abstracts in each of the categories (a total of 10800 documents), and test on the last 100 from each of those four categories (a total of 1200 documents). In class we'll consider metrics to disentangle the performance on multiple classes (using [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html),\n",
    "[confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html), or\n",
    "[plot_confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html)).\n",
    "\n",
    "\n",
    "---\n",
    "**D. (really optional)** If you're already familiar with scikit-learn data formats, then it's instructive (and very quick) to redo the above using either [Bernoulli Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html) (i.e., True/False whether or not word is in document as above), or \n",
    "[Multinomial Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "(using the number counts within documents).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayes' Rule\n",
    "\n",
    "In [lec3.pdf](https://www.cs.cornell.edu/~ginsparg/6010/spr20/lec3.pdf), we saw three examples of Bayes' rule. Here are three simple follow-up questions to reinforce understanding, and form more experience making simple plots.\n",
    "\n",
    "### A. \"occasionally dishonest casino\"\n",
    "We discussed the \"occasionally dishonest casino\" that used two\n",
    "kinds of dice: 99% were fair ($F$), but 1% were loaded ($L$) such that a 6 came up 50%\n",
    "of the time. Thus $p(L)=1/100$, and the conditional probabilities are\n",
    "$p(6\\,|\\,L)=1/2$ and $p(6\\,|\\,F)=1/6$. If we select a die at random and roll three 6's in a row, we saw that the posterior probability $p(L\\,|\\,6^3)$ that it was loaded was only 3/14.\n",
    "\n",
    "**a)**  How many sixes in a row would we have to roll before concluding it\n",
    "was more likely ($>50\\%$ probability) to have been a loaded die?\n",
    "\n",
    "**b)** In your notebook, plot a graph of $p(L\\,|\\, 6^n)$ for $n$ ranging from 0 to 9. (remember to label the axes, and to give it a title)\n",
    "\n",
    "**c)** Suppose that the loaded die has instead $p(6|L)=1/3$, but with $p(L)=.1$ (only 90% of the dice now fair). What would $p(L\\,|\\,6^3)$ be in this case?  Add a plot for the new $p(L\\,|\\, 6^n)$ to the same graph as in part b).\n",
    "\n",
    "### B. Duchenne Muscular Dystrophy\n",
    "We also discussed Duchenne Muscular Dystrophy (DMD),\n",
    "regarded as a simple recessive sex-linked disease caused by a mutated X chromosome (X&#771;).\n",
    "An X&#771;Y male expresses disease, whereas an X&#771;X female is a carrier but does not\n",
    "express the disease. If neither of a woman's parents expresses the disease, but\n",
    "her brother does, then the woman's mother must be a carrier, and the woman\n",
    "herself has an *a priori* 50/50 chance of being a carrier, $p(C)=1/2$.\n",
    "We saw that if she has had a single healthy son, then her probability of being a\n",
    "carrier drops to $p(C\\,|\\,1 {\\rm h.s.})=1/3$.\n",
    "\n",
    "**a)** What is the woman's probability $p(C\\,|\\,n\\ {\\rm h.s.})$ of being a carrier if she gives birth to $n$ healthy sons ($n$ h.s.)?\n",
    "\n",
    "**b)** For how many healthy sons does her probability of being a carrier fall below 1%?\n",
    "\n",
    "**c)** In your notebook, plot a graph of $p(C\\,|\\,n\\ {\\rm h.s.})$ for $n$ from 0 to 9. (remember to label the axes, and to give it a title)\n",
    "\n",
    "\n",
    "### C. Lie detector\n",
    "We discussed briefly (with more detail in lec3 slids) the case of the probability of having a rare disease given a positive test result. A similar question involves the administration of lie detector tests at a hypothetical national laboratory (where the course instructor was formerly employed).\n",
    "We can generously assume that these tests have a 85% sensitivity, i.e., the probability of a spy failing the test is $p(f\\,|\\,S)=.85$, and equally generously can assume that the tests have a false positive rate of only 25%, i.e., the probability of a non-spy failing the test is $p(f\\,|\\,\\overline S)=.25$. We assume that roughly one in ten thousand laboratory employees is a spy, $p(S)=10^{-4}$.\n",
    "\n",
    "**a)** What is the probability $p(S\\,|\\,f)$ that someone who has failed the test is a spy?\n",
    "\n",
    "**b)** Although current empirical data suggests $p(f\\,|\\,S)=0$ (all known spies have been discovered by other means, and regularly passed lie detector tests), let's assume the opposite extreme, that $p(f\\,|\\,S)=1$, i.e., the test is 100% sensitive.\n",
    "How does that affect the result of part a)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
