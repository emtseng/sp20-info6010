{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Info 6010 Ex5\n",
    "\n",
    "**1.A) due Fri 24 Apr 2020 23:59:59, remainder due Thu 30 Apr 2020 23:59:59**\n",
    "\n",
    "Rather than working from this notebook, it's best if you start a new notebook.\n",
    "The first cell should be a markdown cell containing your name (use the menu button 2nd from right above to change from code to markdown).<br>\n",
    "Then you should transfer only the cells you need into the new notebook, so they're not lost in the explanatory text below.\n",
    "\n",
    "Upload to: [upload site](https://pgcourse.infosci.cornell.edu/cgi-bin/probset.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Stylometrics\n",
    "\n",
    "In lec 10 (see [lec10_sty.ipynb](https://nbviewer.jupyter.org/url/www.cs.cornell.edu/~ginsparg/6010/spr20/lec10_sty.ipynb)),\n",
    "we saw how to dimensionally reduce 50 dimensional data to build a k-means classifier to distinguished two authors, Shakespeare and Milton.  The objective here is to repeat those steps with two more authors from `nltk.corpus.gutenberg`: Austen and Melville.\n",
    "\n",
    "[For this problem and the next, it's convenient to use the preprocessed nltk data, see\n",
    "[nltk_data](http://www.nltk.org/data.html) for retrieval instructions.]\n",
    "\n",
    "**A.** The remainder of this part follows the lec9.ipynb code linked above to create objects\n",
    "`top50`, `M`, `scaler`, `M_scaled`, `pca`, `M_new`, `km` analogous to those in the linked code.\n",
    "\n",
    "In an analog of cell [8], add definitions to import words from the additional two files 'austen-persuasion.txt' and 'melville-moby_dick.txt'. The latter has over 200,000 words, so to keep it commensurate in size, it can be truncated to the first 80000 words. It will then contribute only 16 blocks (of 5000 words) to the training data, just as for Milton and Austen.\n",
    "\n",
    "For features, use the list of top50 words from this set as in cell [10], but now from the four authors' texts. Then create an array M as in cell [11] for all four authors, so it will have 13 + 16 + 16 + 16 = 61 lines, each with 50 entries. Draw a plot, as in cell [13], of all 61 lines (using four separate colors).\n",
    "Initialize a `scaler` and create an `M_scaled` array, as in cell [14], then dimensionsionally reduce to 3 dimensions by modifying cell [16], and draw a 3d plot of the data points, as shown in the last slide of [lec10_sty.pdf](https://www.cs.cornell.edu/~ginsparg/6010/spr20/lec10_sty.pdf).\n",
    "\n",
    "**B.** Define a k-means classifier as in cell [20] but initialized to find 4 clusters.\n",
    "Then see how it performs on some test data.\n",
    "For test data, use: 1) the remaining portion of Melville (i.e., from word 80000 on, there should be 27 more blocks of 5000 words); 2) Also try the combined 'austen-emma.txt' and 'austen-sense.txt' (56 more blocks of 5000 words); 3) `nltk.corpus.shakespeare` contains eight more Shakespeare plays (another 39 blocks of 5000 words);  4) Also try some other Milton text, e.g., *Paradise Regained* from Gutenberg.org:\n",
    "\n",
    "    paradise_r_url = 'http://www.gutenberg.org/cache/epub/58/pg58.txt'\n",
    "    paradise_r = urlopen(paradise_r_url).read().decode('utf-8')\n",
    "    paradise_r_words = nltk.word_tokenize(paradise_r[540:100260])\n",
    "\n",
    "That or any other texts from the web would have to be tokenized (see [nltk_ch02](http://www.nltk.org/book/ch02.html) re accessing web corpora), which can be done using tools from earlier exercises (e.g., the regular expression `[\\w\\']+`, or using the nltk tool as above (`nltk.word_tokenize()` is described in [nltk_ch03](http://www.nltk.org/book/ch03.html), remember afterwards to lower case and remove punctuation). The above limits on `paradise_r` strip off the Gutenburg header/trailer material, leaving three 5000 words blocks.\n",
    "\n",
    "See if any of the 27 + 56 + 39 + 3 = 125 blocks of test data are classified incorrectly by the k-means classifier, and redraw the plot above with the above points added (with smaller markersize for the test data, and colored according to their classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)\n",
    "In class lec11 (24 Apr), we discussed briefly the issue of algorithms used to help determine whether a defendant should be granted bail, specifically the COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) scores based on [137 questions](https://www.documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPAS-CORE.html)\n",
    "used to determine \"recidivism risk\" (i.e., likelihood of committing another crime), \"violent recidivism risk\" (likelihood of committing a violent crime), and \"flight risk\" (likelihood of not showing up for trial).\n",
    "\n",
    "The issue of machine bias in was considered in a series of articles, including \n",
    "[ProPublica (May 2016)](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing), with associated [ProPublica Analysis](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm)\n",
    "and the [Propublica data](https://github.com/propublica/compas-analysis), and this follow-up: [ProPublica Aug 2016](https://www.propublica.org/article/making-algorithms-accountable).\n",
    "\n",
    "Concerns were soon raised that the bias was not as unambiguous as claimed\n",
    "[WaPo (oct 2016)](https://www.washingtonpost.com/news/monkey-cage/wp/2016/10/17/can-an-algorithm-be-racist-our-analysis-is-more-cautious-than-propublicas/), and even mathematically inevitable [ProPublica (Dec 2016)](https://www.propublica.org/article/bias-in-criminal-risk-scores-is-mathematically-inevitable-researchers-say). (Some of these results were due to Kleinberg et al. [2016](https://arxiv.org/pdf/1609.05807.pdf), \n",
    "[2017](https://cs.stanford.edu/people/jure/pubs/bail-qje17.pdf), and\n",
    "[2018](https://www.cs.cornell.edu/home/kleinber/aer18-fairness.pdf) -- these have technical parts, but also accessible introductory discussions and conclusions.)\n",
    "\n",
    "The above are all encouraged reading, but this problem will investigate a variant of a slightly different aspect, as considered here: [The Atlantic (Jan 2018)](https://www.theatlantic.com/technology/archive/2018/01/equivant-compas-algorithm/550646/).\n",
    "\n",
    "[Ex5_supp.ipynb](http://nbviewer.jupyter.org/url/www.cs.cornell.edu/~ginsparg/6010/spr20/Ex5_supp.ipynb) shows how to take one of the datafiles from the \n",
    "[Propublica data](https://github.com/propublica/compas-analysis) and extract a subset\n",
    "[parsed_compas-scores.csv](https://www.cs.cornell.edu/~ginsparg/6010/spr20/parsed_compas-scores.csv)\n",
    "with 11742 data rows, 12 features, and two of the COMPAS scores (for \"recividism risk\" and \"violent recividism risk\", using three categories: \"low\", \"medium\", and \"high\").\n",
    "[Note that one of the features available in the data is \"race\", though that was for the Propublica assessment, and not originally used by the COMPAS algorithm.]\n",
    "\n",
    "You can just download the above `parsed_compas-scores.csv` without repeating the process, though it might be worth skimming to see what the original data looked like.\n",
    "\n",
    "Cells [9-12] of [lec11_dt.ipynb](https://nbviewer.jupyter.org/url/www.cs.cornell.edu/~ginsparg/6010/spr20/lec11_dt.ipynb) demo the use of `sklearn.tree.DecisionTreeClassifier()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A.** Use the sklearn `tree.DecisionTreeClassifier()` to train decision trees from the 12 features in [parsed_compas-scores.csv](https://www.cs.cornell.edu/~ginsparg/6010/spr20/parsed_compas-scores.csv) to predict the values for `v_score` and `r_score` (violent and overall recividism risks).\n",
    "\n",
    "i) First train on the full 11742 entry dataset, just to determine the overall ability to fit the data. What is the overall training set accuracy in each case? (Why is it not 100%?)\n",
    "\n",
    "ii) What are the top two features returned by the `.feature_importances_` attribute in each of the two cases, and how much of the overall explanatory power do they contribute?\n",
    "\n",
    "iii) How much does race contribute to the overall explanatory power? (In this case we're using race as a proxy for missing features that may be correlated with it)\n",
    "\n",
    "iv) Plot a decision tree for each case: note that since these trees are very deep, you should use\n",
    "`max_depth=2` as an argument to `tree.plot_tree()` (otherwise it might hang); also you should use the `feature_names=` and `class_names=` parameters to make it easier to read (setting `fontsize=` larger might also help), and `filled=True` will indicate the classes by color.\n",
    "\n",
    "**B.** Then use sklearn.model_selection `cross_val_score` to perform a ten-fold cross-validation (dicussed in class lec20). This will automatically partition the data into a set of training and test sets (\"stratified k-fold\"), and returns a list of test scores for successive 10% partitions not included in the training sets.\n",
    "\n",
    "How do these test scores compare to the training set accuracies for the two cases?<br>\n",
    "(And how do they compare to choosing randomly, knowing the relative sizes of the three classes?)\n",
    "\n",
    "**C.** i) Train on just the two most important features for each case, determined as in part A.ii.<br>\n",
    "How does it score on the full training set as in A, how well under 10-fold cross-validation as in B?\n",
    "\n",
    "ii) Train on just the three most important features for each case, and again determine the score on the full training set as in A, and under 10-fold cross-validation as in B.\n",
    "\n",
    "**D.** The 11742 sets of features in the above contain many repeats, sometimes with inconsistent values for `v_score` or for `r_score`. Reduce this to a dataset of just the 2638 distinct values for the 12 features, using majority rule to determine the value of each of the features (`np.mode()` is useful for this), and repeat A, B on this smaller dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)\n",
    "This was originally going to be #2, but is now a bonus, since Ex6 has to be shorter due to the lost week, and the new #2 above would have been prob1 on Ex6.\n",
    "[To be added 25 Apr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
